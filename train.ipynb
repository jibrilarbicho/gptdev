{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c9542373-6611-4b24-a6c7-2979563b4e53/gptdev/gptenv/lib/python3.11/site-packages/torch/_subclasses/functional_tensor.py:275: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[57,  1, 46, 47, 57,  1, 50, 53],\n",
      "        [ 1, 58, 46, 43, 56, 43,  1, 41],\n",
      "        [17, 26, 15, 17, 10,  0, 32, 53],\n",
      "        [57, 58,  6,  1, 61, 47, 58, 46]])\n",
      "tensor([[ 1, 46, 47, 57,  1, 50, 53, 60],\n",
      "        [58, 46, 43, 56, 43,  1, 41, 39],\n",
      "        [26, 15, 17, 10,  0, 32, 53,  1],\n",
      "        [58,  6,  1, 61, 47, 58, 46,  0]])\n",
      "context: tensor([57]), target: 1\n",
      "context: tensor([57,  1]), target: 46\n",
      "context: tensor([57,  1, 46]), target: 47\n",
      "context: tensor([57,  1, 46, 47]), target: 57\n",
      "context: tensor([57,  1, 46, 47, 57]), target: 1\n",
      "context: tensor([57,  1, 46, 47, 57,  1]), target: 50\n",
      "context: tensor([57,  1, 46, 47, 57,  1, 50]), target: 53\n",
      "context: tensor([57,  1, 46, 47, 57,  1, 50, 53]), target: 60\n",
      "context: tensor([1]), target: 58\n",
      "context: tensor([ 1, 58]), target: 46\n",
      "context: tensor([ 1, 58, 46]), target: 43\n",
      "context: tensor([ 1, 58, 46, 43]), target: 56\n",
      "context: tensor([ 1, 58, 46, 43, 56]), target: 43\n",
      "context: tensor([ 1, 58, 46, 43, 56, 43]), target: 1\n",
      "context: tensor([ 1, 58, 46, 43, 56, 43,  1]), target: 41\n",
      "context: tensor([ 1, 58, 46, 43, 56, 43,  1, 41]), target: 39\n",
      "context: tensor([17]), target: 26\n",
      "context: tensor([17, 26]), target: 15\n",
      "context: tensor([17, 26, 15]), target: 17\n",
      "context: tensor([17, 26, 15, 17]), target: 10\n",
      "context: tensor([17, 26, 15, 17, 10]), target: 0\n",
      "context: tensor([17, 26, 15, 17, 10,  0]), target: 32\n",
      "context: tensor([17, 26, 15, 17, 10,  0, 32]), target: 53\n",
      "context: tensor([17, 26, 15, 17, 10,  0, 32, 53]), target: 1\n",
      "context: tensor([57]), target: 58\n",
      "context: tensor([57, 58]), target: 6\n",
      "context: tensor([57, 58,  6]), target: 1\n",
      "context: tensor([57, 58,  6,  1]), target: 61\n",
      "context: tensor([57, 58,  6,  1, 61]), target: 47\n",
      "context: tensor([57, 58,  6,  1, 61, 47]), target: 58\n",
      "context: tensor([57, 58,  6,  1, 61, 47, 58]), target: 46\n",
      "context: tensor([57, 58,  6,  1, 61, 47, 58, 46]), target: 0\n"
     ]
    }
   ],
   "source": [
    "with open(\"./input.txt\", \"r\",encoding=\"utf-8\") as file:\n",
    "    data = file.read()\n",
    "    # print(data)\n",
    "import torch\n",
    "\n",
    "voc=sorted(list(set(data)))\n",
    "# print(len(data))\n",
    "# print(list(set(data)))\n",
    "# print(set(data))\n",
    "# print(voc)\n",
    "chtoi={c:i for i,c in enumerate(voc)}\n",
    "itoch={i:c for i,c in enumerate(voc)}\n",
    "encoding=lambda s: [chtoi[c] for c in s]\n",
    "decoding=lambda s: \"\".join([itoch[c] for c in s])\n",
    "\n",
    "\n",
    "# print(encoding(\"hello world\"))\n",
    "# print(decoding(encoding(\"hello world\")))\n",
    "\n",
    "datatset=torch.tensor(encoding(data),dtype=torch.long)\n",
    "# print(datatset[:100])\n",
    "n=int(0.9*len(datatset))\n",
    "trainset=datatset[:n]\n",
    "testset=datatset[n:]\n",
    "block_size=8\n",
    "x_train=datatset[:block_size+1]\n",
    "y_train=datatset[1:block_size+1]\n",
    "# print(x_train)\n",
    "for i in range(block_size):\n",
    "    context=x_train[:i+1]\n",
    "    target=y_train[i]\n",
    "    # print(f\"context: {context}, target: {target}\")\n",
    "torch.manual_seed(42)\n",
    "batch_size=4\n",
    "block_size=8\n",
    "def getbatch(split):\n",
    "    data=trainset if split==\"train\" else testset\n",
    "    ix=torch.randint(len(data)-block_size,(batch_size,))\n",
    "    x=torch.stack(([data[i:i+block_size] for i in ix]))\n",
    "    y=torch.stack(([data[i+1:i+block_size+1] for i in ix]))\n",
    "    # print(ix)\n",
    "    return x,y\n",
    "xb,yb=getbatch(\"train\")\n",
    "print(xb)\n",
    "print(yb)\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for a in range(block_size):\n",
    "        context=xb[b,:a+1]\n",
    "        target=yb[b,a]\n",
    "        print(f\"context: {context}, target: {target}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7bc214375ff0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "uoiaF$z\n",
      "M?kI;h\n",
      "DbuMG,H3LYNmrDxKgTpvAKOF-jU.hc;fBMTGa-IS\n",
      "g3lEb&ZQ,l;:m;lpcNN\n",
      "KpVEYRIIM,'hCRbMAcWTkrnH\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table=nn.Embedding(vocab_size,vocab_size)\n",
    "    def forward(self, xb,yb=None):\n",
    "        logits=self.token_embedding_table(xb)\n",
    "      \n",
    "        if yb is not None:\n",
    "            B,C,T=logits.shape\n",
    "            logits=logits.view(B*C,T)\n",
    "            yb=yb.view(B*C)\n",
    "\n",
    "            loss=F.cross_entropy(logits,yb)\n",
    "        else:\n",
    "            loss=None\n",
    "        return logits,loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens): \n",
    "        # idx is (B, T) array of indices in the current context \n",
    "        for _ in range(max_new_tokens):  \n",
    "            # get the predictions \n",
    "            logits, loss = self(idx)  \n",
    "\n",
    "            # focus only on the last time step \n",
    "            # print(logits)\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)  \n",
    "            # print(logits)\n",
    "\n",
    "            # apply softmax to get probabilities  \n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)  \n",
    "\n",
    "            # sample from the distribution  \n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)  \n",
    "\n",
    "            # append sampled index to the running sequence  \n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1) \n",
    "            # print(idx) \n",
    "\n",
    "        return idx\n",
    "\n",
    "m=BigramLanguageModel(len(voc))\n",
    "logit,loss=m(xb,yb)\n",
    "# print(logit.shape)\n",
    "# print(loss)\n",
    "        \n",
    "# print(len(voc))\n",
    "\n",
    "print(decoding(m.generate(idx=torch.zeros((1,1),dtype=torch.long),max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0, 16, 23, 34, 39, 51,  4, 23,  9, 38, 52, 49, 21, 25, 39, 55, 52, 58,\n",
       "         48, 46,  7, 13, 35,  5, 14, 22, 62, 62, 45, 46, 48, 38, 32, 14, 15, 52,\n",
       "         15, 33, 12, 55, 14, 29, 40, 61, 63, 15, 48, 46, 30, 46, 31, 24, 63, 22,\n",
       "         44, 19, 58, 57, 59, 58, 22, 14, 63,  7, 48,  4,  6, 56,  6,  5, 49,  3,\n",
       "         28, 32, 59, 38, 51,  9, 58, 28, 40, 59,  1, 26, 61, 20,  7, 21, 28, 43,\n",
       "         39, 55, 59, 22, 34, 56, 23,  5, 62,  1, 44]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.generate(idx=torch.zeros((1,1),dtype=torch.long),max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"\"\"gBXCYUEBUkTrhlwhYvCHU3jVQWIOVFHSbBtlnSfFoHZK.nHR&AcjvfzzJ.K,HCYUui-xd\n",
    "L;Wu3VnSgv.$afs\n",
    "BqpdHVLgwcOwN \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.AdamW(m.parameters(),lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Loss: 4.6977\n",
      "Step 101, Loss: 4.5542\n",
      "Step 201, Loss: 4.4854\n",
      "Step 301, Loss: 4.3880\n",
      "Step 401, Loss: 4.2180\n",
      "Step 501, Loss: 4.2192\n",
      "Step 601, Loss: 4.0598\n",
      "Step 701, Loss: 3.9743\n",
      "Step 801, Loss: 3.9993\n",
      "Step 901, Loss: 3.9300\n",
      "Step 1001, Loss: 3.7942\n",
      "Step 1101, Loss: 3.8274\n",
      "Step 1201, Loss: 3.5697\n",
      "Step 1301, Loss: 3.4974\n",
      "Step 1401, Loss: 3.5198\n",
      "Step 1501, Loss: 3.3823\n",
      "Step 1601, Loss: 3.4163\n",
      "Step 1701, Loss: 3.3589\n",
      "Step 1801, Loss: 3.3072\n",
      "Step 1901, Loss: 3.2026\n",
      "Step 2001, Loss: 3.1628\n",
      "Step 2101, Loss: 3.0630\n",
      "Step 2201, Loss: 3.1235\n",
      "Step 2301, Loss: 2.9732\n",
      "Step 2401, Loss: 3.0888\n",
      "Step 2501, Loss: 2.9927\n",
      "Step 2601, Loss: 2.8689\n",
      "Step 2701, Loss: 2.9053\n",
      "Step 2801, Loss: 2.9279\n",
      "Step 2901, Loss: 2.8635\n",
      "Step 3001, Loss: 2.8600\n",
      "Step 3101, Loss: 2.9175\n",
      "Step 3201, Loss: 2.7034\n",
      "Step 3301, Loss: 2.7863\n",
      "Step 3401, Loss: 2.7986\n",
      "Step 3501, Loss: 2.6293\n",
      "Step 3601, Loss: 2.7755\n",
      "Step 3701, Loss: 2.7048\n",
      "Step 3801, Loss: 2.6091\n",
      "Step 3901, Loss: 2.5815\n",
      "Step 4001, Loss: 2.6208\n",
      "Step 4101, Loss: 2.6359\n",
      "Step 4201, Loss: 2.6495\n",
      "Step 4301, Loss: 2.6701\n",
      "Step 4401, Loss: 2.6540\n",
      "Step 4501, Loss: 2.6137\n",
      "Step 4601, Loss: 2.5385\n",
      "Step 4701, Loss: 2.7403\n",
      "Step 4801, Loss: 2.5221\n",
      "Step 4901, Loss: 2.6266\n",
      "Step 5001, Loss: 2.5505\n",
      "Step 5101, Loss: 2.5610\n",
      "Step 5201, Loss: 2.5470\n",
      "Step 5301, Loss: 2.5149\n",
      "Step 5401, Loss: 2.5262\n",
      "Step 5501, Loss: 2.4623\n",
      "Step 5601, Loss: 2.5615\n",
      "Step 5701, Loss: 2.6202\n",
      "Step 5801, Loss: 2.5995\n",
      "Step 5901, Loss: 2.5212\n",
      "Step 6001, Loss: 2.5571\n",
      "Step 6101, Loss: 2.5108\n",
      "Step 6201, Loss: 2.5454\n",
      "Step 6301, Loss: 2.4263\n",
      "Step 6401, Loss: 2.4940\n",
      "Step 6501, Loss: 2.4318\n",
      "Step 6601, Loss: 2.6770\n",
      "Step 6701, Loss: 2.5536\n",
      "Step 6801, Loss: 2.4955\n",
      "Step 6901, Loss: 2.4084\n",
      "Step 7001, Loss: 2.3854\n",
      "Step 7101, Loss: 2.5597\n",
      "Step 7201, Loss: 2.5068\n",
      "Step 7301, Loss: 2.4465\n",
      "Step 7401, Loss: 2.4949\n",
      "Step 7501, Loss: 2.3700\n",
      "Step 7601, Loss: 2.4280\n",
      "Step 7701, Loss: 2.4072\n",
      "Step 7801, Loss: 2.3597\n",
      "Step 7901, Loss: 2.4519\n",
      "Step 8001, Loss: 2.5282\n",
      "Step 8101, Loss: 2.4408\n",
      "Step 8201, Loss: 2.5789\n",
      "Step 8301, Loss: 2.4648\n",
      "Step 8401, Loss: 2.5081\n",
      "Step 8501, Loss: 2.5864\n",
      "Step 8601, Loss: 2.4633\n",
      "Step 8701, Loss: 2.5087\n",
      "Step 8801, Loss: 2.4265\n",
      "Step 8901, Loss: 2.5012\n",
      "Step 9001, Loss: 2.4003\n",
      "Step 9101, Loss: 2.4489\n",
      "Step 9201, Loss: 2.4928\n",
      "Step 9301, Loss: 2.4946\n",
      "Step 9401, Loss: 2.4609\n",
      "Step 9501, Loss: 2.4100\n",
      "Step 9601, Loss: 2.4860\n",
      "Step 9701, Loss: 2.4334\n",
      "Step 9801, Loss: 2.5789\n",
      "Step 9901, Loss: 2.5258\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "for steps in range(10000):\n",
    "    xb, yb = getbatch('train')\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if steps % 100 == 0:\n",
    "        print(f\"Step {steps+1}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RY:\n",
      "Yors t p kere isd, m INGea earusher Tistced.\n",
      "Wharologo!\n",
      "\n",
      "\n",
      "Whe hildy por byof?\n",
      "GUFarastaldXIndesw\n"
     ]
    }
   ],
   "source": [
    "print(decoding(m.generate(idx=torch.zeros((1,1),dtype=torch.long),max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.1583,  1.0940],\n",
      "         [-0.1402,  0.1227],\n",
      "         [-1.7025,  0.6909],\n",
      "         [-0.5811,  1.5571],\n",
      "         [-0.0903, -0.4272],\n",
      "         [ 1.1420,  1.8703],\n",
      "         [-0.5932, -1.6216],\n",
      "         [ 0.0837,  0.4461]],\n",
      "\n",
      "        [[-0.7926, -1.6044],\n",
      "         [ 0.3674, -0.3286],\n",
      "         [-1.3446,  0.2414],\n",
      "         [-1.3129,  0.1608],\n",
      "         [ 1.4516, -2.6770],\n",
      "         [ 1.0325,  0.0754],\n",
      "         [-1.2991, -0.3834],\n",
      "         [-0.0684, -3.1754]],\n",
      "\n",
      "        [[ 0.2173,  1.4930],\n",
      "         [-2.7524,  1.2803],\n",
      "         [-0.2602, -1.3573],\n",
      "         [ 0.1989,  1.4771],\n",
      "         [-0.8706, -1.1885],\n",
      "         [ 0.9991, -0.3465],\n",
      "         [-0.6703,  0.1645],\n",
      "         [ 0.6289, -0.3070]],\n",
      "\n",
      "        [[ 1.2607,  0.2722],\n",
      "         [ 0.4268, -0.3683],\n",
      "         [-0.9086,  1.8521],\n",
      "         [ 2.4499, -1.9422],\n",
      "         [ 0.4257,  0.0806],\n",
      "         [ 0.5172,  0.9361],\n",
      "         [-0.0533,  0.8518],\n",
      "         [ 0.7028,  0.8040]]])\n"
     ]
    }
   ],
   "source": [
    "B,T,C=4,8,2\n",
    "x=torch.randn(B,T,C)\n",
    "xbow=torch.zeros(B,T,C)\n",
    "print(x)\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev=x[b,:t+1]\n",
    "        xbow[b,t]=torch.mean(xprev,dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.1583,  1.0940],\n",
       "         [-0.1402,  0.1227],\n",
       "         [-1.7025,  0.6909],\n",
       "         [-0.5811,  1.5571],\n",
       "         [-0.0903, -0.4272],\n",
       "         [ 1.1420,  1.8703],\n",
       "         [-0.5932, -1.6216],\n",
       "         [ 0.0837,  0.4461]]),\n",
       " tensor([[ 1.1583,  1.0940],\n",
       "         [ 0.5091,  0.6083],\n",
       "         [-0.2281,  0.6359],\n",
       "         [-0.3164,  0.8662],\n",
       "         [-0.2712,  0.6075],\n",
       "         [-0.0356,  0.8180],\n",
       "         [-0.1153,  0.4695],\n",
       "         [-0.0904,  0.4665]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0],xbow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "a=torch.tril(torch.ones(3,3,dtype=torch.float))\n",
    "b=torch.randint(0,10,(3,2),dtype=torch.float)\n",
    "c=a@b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 0., 0.],\n",
       "         [1., 1., 0.],\n",
       "         [1., 1., 1.]]),\n",
       " tensor([[2., 7.],\n",
       "         [6., 4.],\n",
       "         [6., 5.]]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=a/torch.sum(a,dim=1,keepdim=True)\n",
    "# b=torch.sum(a,dim=1)\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v=torch.tril(torch.ones(T,T))\n",
    "v=v/torch.sum(v,dim=1,keepdim=True)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# s=v/torch.sum(v,dim=1)\n",
    "# s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gptenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
