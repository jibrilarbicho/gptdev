{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[57,  1, 46, 47, 57,  1, 50, 53],\n",
      "        [ 1, 58, 46, 43, 56, 43,  1, 41],\n",
      "        [17, 26, 15, 17, 10,  0, 32, 53],\n",
      "        [57, 58,  6,  1, 61, 47, 58, 46]])\n",
      "tensor([[ 1, 46, 47, 57,  1, 50, 53, 60],\n",
      "        [58, 46, 43, 56, 43,  1, 41, 39],\n",
      "        [26, 15, 17, 10,  0, 32, 53,  1],\n",
      "        [58,  6,  1, 61, 47, 58, 46,  0]])\n",
      "context: tensor([57]), target: 1\n",
      "context: tensor([57,  1]), target: 46\n",
      "context: tensor([57,  1, 46]), target: 47\n",
      "context: tensor([57,  1, 46, 47]), target: 57\n",
      "context: tensor([57,  1, 46, 47, 57]), target: 1\n",
      "context: tensor([57,  1, 46, 47, 57,  1]), target: 50\n",
      "context: tensor([57,  1, 46, 47, 57,  1, 50]), target: 53\n",
      "context: tensor([57,  1, 46, 47, 57,  1, 50, 53]), target: 60\n",
      "context: tensor([1]), target: 58\n",
      "context: tensor([ 1, 58]), target: 46\n",
      "context: tensor([ 1, 58, 46]), target: 43\n",
      "context: tensor([ 1, 58, 46, 43]), target: 56\n",
      "context: tensor([ 1, 58, 46, 43, 56]), target: 43\n",
      "context: tensor([ 1, 58, 46, 43, 56, 43]), target: 1\n",
      "context: tensor([ 1, 58, 46, 43, 56, 43,  1]), target: 41\n",
      "context: tensor([ 1, 58, 46, 43, 56, 43,  1, 41]), target: 39\n",
      "context: tensor([17]), target: 26\n",
      "context: tensor([17, 26]), target: 15\n",
      "context: tensor([17, 26, 15]), target: 17\n",
      "context: tensor([17, 26, 15, 17]), target: 10\n",
      "context: tensor([17, 26, 15, 17, 10]), target: 0\n",
      "context: tensor([17, 26, 15, 17, 10,  0]), target: 32\n",
      "context: tensor([17, 26, 15, 17, 10,  0, 32]), target: 53\n",
      "context: tensor([17, 26, 15, 17, 10,  0, 32, 53]), target: 1\n",
      "context: tensor([57]), target: 58\n",
      "context: tensor([57, 58]), target: 6\n",
      "context: tensor([57, 58,  6]), target: 1\n",
      "context: tensor([57, 58,  6,  1]), target: 61\n",
      "context: tensor([57, 58,  6,  1, 61]), target: 47\n",
      "context: tensor([57, 58,  6,  1, 61, 47]), target: 58\n",
      "context: tensor([57, 58,  6,  1, 61, 47, 58]), target: 46\n",
      "context: tensor([57, 58,  6,  1, 61, 47, 58, 46]), target: 0\n"
     ]
    }
   ],
   "source": [
    "with open(\"./input.txt\", \"r\",encoding=\"utf-8\") as file:\n",
    "    data = file.read()\n",
    "    # print(data)\n",
    "import torch\n",
    "\n",
    "voc=sorted(list(set(data)))\n",
    "# print(len(data))\n",
    "# print(list(set(data)))\n",
    "# print(set(data))\n",
    "# print(voc)\n",
    "chtoi={c:i for i,c in enumerate(voc)}\n",
    "itoch={i:c for i,c in enumerate(voc)}\n",
    "encoding=lambda s: [chtoi[c] for c in s]\n",
    "decoding=lambda s: \"\".join([itoch[c] for c in s])\n",
    "\n",
    "\n",
    "# print(encoding(\"hello world\"))\n",
    "# print(decoding(encoding(\"hello world\")))\n",
    "\n",
    "datatset=torch.tensor(encoding(data),dtype=torch.long)\n",
    "# print(datatset[:100])\n",
    "n=int(0.9*len(datatset))\n",
    "trainset=datatset[:n]\n",
    "testset=datatset[n:]\n",
    "block_size=8\n",
    "x_train=datatset[:block_size+1]\n",
    "y_train=datatset[1:block_size+1]\n",
    "# print(x_train)\n",
    "for i in range(block_size):\n",
    "    context=x_train[:i+1]\n",
    "    target=y_train[i]\n",
    "    # print(f\"context: {context}, target: {target}\")\n",
    "torch.manual_seed(42)\n",
    "batch_size=4\n",
    "block_size=8\n",
    "def getbatch(split):\n",
    "    data=trainset if split==\"train\" else testset\n",
    "    ix=torch.randint(len(data)-block_size,(batch_size,))\n",
    "    x=torch.stack(([data[i:i+block_size] for i in ix]))\n",
    "    y=torch.stack(([data[i+1:i+block_size+1] for i in ix]))\n",
    "    # print(ix)\n",
    "    return x,y\n",
    "xb,yb=getbatch(\"train\")\n",
    "print(xb)\n",
    "print(yb)\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for a in range(block_size):\n",
    "        context=xb[b,:a+1]\n",
    "        target=yb[b,a]\n",
    "        print(f\"context: {context}, target: {target}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x79f37418e730>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "uoiaF$z\n",
      "M?kI;h\n",
      "DbuMG,H3LYNmrDxKgTpvAKOF-jU.hc;fBMTGa-IS\n",
      "g3lEb&ZQ,l;:m;lpcNN\n",
      "KpVEYRIIM,'hCRbMAcWTkrnH\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table=nn.Embedding(vocab_size,vocab_size)\n",
    "    def forward(self, xb,yb=None):\n",
    "        logits=self.token_embedding_table(xb)\n",
    "      \n",
    "        if yb is not None:\n",
    "            B,C,T=logits.shape\n",
    "            logits=logits.view(B*C,T)\n",
    "            yb=yb.view(B*C)\n",
    "\n",
    "            loss=F.cross_entropy(logits,yb)\n",
    "        else:\n",
    "            loss=None\n",
    "        return logits,loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens): \n",
    "        # idx is (B, T) array of indices in the current context \n",
    "        for _ in range(max_new_tokens):  \n",
    "            # get the predictions \n",
    "            logits, loss = self(idx)  \n",
    "\n",
    "            # focus only on the last time step \n",
    "            # print(logits)\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)  \n",
    "            # print(logits)\n",
    "\n",
    "            # apply softmax to get probabilities  \n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)  \n",
    "\n",
    "            # sample from the distribution  \n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)  \n",
    "\n",
    "            # append sampled index to the running sequence  \n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1) \n",
    "            # print(idx) \n",
    "\n",
    "        return idx\n",
    "\n",
    "m=BigramLanguageModel(len(voc))\n",
    "logit,loss=m(xb,yb)\n",
    "# print(logit.shape)\n",
    "# print(loss)\n",
    "        \n",
    "# print(len(voc))\n",
    "\n",
    "print(decoding(m.generate(idx=torch.zeros((1,1),dtype=torch.long),max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0, 15, 39, 52, 46, 40, 39, 44, 64, 48, 35, 56,  6, 25, 60, 47, 39,  1,\n",
       "         58,  5, 28, 43, 50,  5, 42,  1,  1, 43,  1, 63, 46, 39,  9, 52,  0, 58,\n",
       "         43, 17, 61, 44, 18, 24, 17, 21, 25, 13, 25, 28, 20, 20, 17, 31, 49, 12,\n",
       "          7, 40, 53, 53, 59, 51, 39, 51, 32, 47, 39,  7, 21, 52, 43, 51, 39, 42,\n",
       "          2, 41,  8,  0, 24, 32,  1, 17, 35, 20, 24, 14,  2,  4,  6,  1, 58, 59,\n",
       "         25, 60, 49, 12, 61, 33,  1, 57, 57, 47, 53]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.generate(idx=torch.zeros((1,1),dtype=torch.long),max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"\"\"gBXCYUEBUkTrhlwhYvCHU3jVQWIOVFHSbBtlnSfFoHZK.nHR&AcjvfzzJ.K,HCYUui-xd\n",
    "L;Wu3VnSgv.$afs\n",
    "BqpdHVLgwcOwN \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.AdamW(m.parameters(),lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Loss: 3.1633\n",
      "Step 101, Loss: 3.0629\n",
      "Step 201, Loss: 3.1231\n",
      "Step 301, Loss: 2.9733\n",
      "Step 401, Loss: 3.0888\n",
      "Step 501, Loss: 2.9932\n",
      "Step 601, Loss: 2.8694\n",
      "Step 701, Loss: 2.9049\n",
      "Step 801, Loss: 2.9283\n",
      "Step 901, Loss: 2.8630\n",
      "Step 1001, Loss: 2.8598\n",
      "Step 1101, Loss: 2.9170\n",
      "Step 1201, Loss: 2.7032\n",
      "Step 1301, Loss: 2.7869\n",
      "Step 1401, Loss: 2.7989\n",
      "Step 1501, Loss: 2.6292\n",
      "Step 1601, Loss: 2.7759\n",
      "Step 1701, Loss: 2.7046\n",
      "Step 1801, Loss: 2.6092\n",
      "Step 1901, Loss: 2.5815\n",
      "Step 2001, Loss: 2.6210\n",
      "Step 2101, Loss: 2.6367\n",
      "Step 2201, Loss: 2.6492\n",
      "Step 2301, Loss: 2.6701\n",
      "Step 2401, Loss: 2.6539\n",
      "Step 2501, Loss: 2.6140\n",
      "Step 2601, Loss: 2.5384\n",
      "Step 2701, Loss: 2.7407\n",
      "Step 2801, Loss: 2.5222\n",
      "Step 2901, Loss: 2.6269\n",
      "Step 3001, Loss: 2.5507\n",
      "Step 3101, Loss: 2.5608\n",
      "Step 3201, Loss: 2.5470\n",
      "Step 3301, Loss: 2.5149\n",
      "Step 3401, Loss: 2.5263\n",
      "Step 3501, Loss: 2.4625\n",
      "Step 3601, Loss: 2.5614\n",
      "Step 3701, Loss: 2.6202\n",
      "Step 3801, Loss: 2.5995\n",
      "Step 3901, Loss: 2.5212\n",
      "Step 4001, Loss: 2.5573\n",
      "Step 4101, Loss: 2.5108\n",
      "Step 4201, Loss: 2.5451\n",
      "Step 4301, Loss: 2.4262\n",
      "Step 4401, Loss: 2.4942\n",
      "Step 4501, Loss: 2.4320\n",
      "Step 4601, Loss: 2.6768\n",
      "Step 4701, Loss: 2.5536\n",
      "Step 4801, Loss: 2.4954\n",
      "Step 4901, Loss: 2.4085\n",
      "Step 5001, Loss: 2.3856\n",
      "Step 5101, Loss: 2.5597\n",
      "Step 5201, Loss: 2.5067\n",
      "Step 5301, Loss: 2.4465\n",
      "Step 5401, Loss: 2.4949\n",
      "Step 5501, Loss: 2.3700\n",
      "Step 5601, Loss: 2.4278\n",
      "Step 5701, Loss: 2.4073\n",
      "Step 5801, Loss: 2.3595\n",
      "Step 5901, Loss: 2.4520\n",
      "Step 6001, Loss: 2.5282\n",
      "Step 6101, Loss: 2.4409\n",
      "Step 6201, Loss: 2.5790\n",
      "Step 6301, Loss: 2.4648\n",
      "Step 6401, Loss: 2.5079\n",
      "Step 6501, Loss: 2.5864\n",
      "Step 6601, Loss: 2.4633\n",
      "Step 6701, Loss: 2.5088\n",
      "Step 6801, Loss: 2.4264\n",
      "Step 6901, Loss: 2.5013\n",
      "Step 7001, Loss: 2.4003\n",
      "Step 7101, Loss: 2.4489\n",
      "Step 7201, Loss: 2.4928\n",
      "Step 7301, Loss: 2.4945\n",
      "Step 7401, Loss: 2.4609\n",
      "Step 7501, Loss: 2.4101\n",
      "Step 7601, Loss: 2.4860\n",
      "Step 7701, Loss: 2.4334\n",
      "Step 7801, Loss: 2.5789\n",
      "Step 7901, Loss: 2.5258\n",
      "Step 8001, Loss: 2.4072\n",
      "Step 8101, Loss: 2.4883\n",
      "Step 8201, Loss: 2.4767\n",
      "Step 8301, Loss: 2.4218\n",
      "Step 8401, Loss: 2.4874\n",
      "Step 8501, Loss: 2.5656\n",
      "Step 8601, Loss: 2.5018\n",
      "Step 8701, Loss: 2.5037\n",
      "Step 8801, Loss: 2.6091\n",
      "Step 8901, Loss: 2.3804\n",
      "Step 9001, Loss: 2.4795\n",
      "Step 9101, Loss: 2.4315\n",
      "Step 9201, Loss: 2.4073\n",
      "Step 9301, Loss: 2.4474\n",
      "Step 9401, Loss: 2.4725\n",
      "Step 9501, Loss: 2.5027\n",
      "Step 9601, Loss: 2.5634\n",
      "Step 9701, Loss: 2.4970\n",
      "Step 9801, Loss: 2.5398\n",
      "Step 9901, Loss: 2.4342\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "for steps in range(10000):\n",
    "    xb, yb = getbatch('train')\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if steps % 100 == 0:\n",
    "        print(f\"Step {steps+1}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decoding(m.generate(idx=torch.zeros((1,1),dtype=torch.long),max_new_tokens=100)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gptenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
