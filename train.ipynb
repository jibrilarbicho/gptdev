{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c9542373-6611-4b24-a6c7-2979563b4e53/gptdev/gptenv/lib/python3.11/site-packages/torch/_subclasses/functional_tensor.py:275: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([476250,  18899, 645194, 831150])\n",
      "tensor([[57,  1, 46, 47, 57,  1, 50, 53],\n",
      "        [ 1, 58, 46, 43, 56, 43,  1, 41],\n",
      "        [17, 26, 15, 17, 10,  0, 32, 53],\n",
      "        [57, 58,  6,  1, 61, 47, 58, 46]])\n",
      "tensor([[ 1, 46, 47, 57,  1, 50, 53, 60],\n",
      "        [58, 46, 43, 56, 43,  1, 41, 39],\n",
      "        [26, 15, 17, 10,  0, 32, 53,  1],\n",
      "        [58,  6,  1, 61, 47, 58, 46,  0]])\n",
      "context: tensor([57]), target: 1\n",
      "context: tensor([57,  1]), target: 46\n",
      "context: tensor([57,  1, 46]), target: 47\n",
      "context: tensor([57,  1, 46, 47]), target: 57\n",
      "context: tensor([57,  1, 46, 47, 57]), target: 1\n",
      "context: tensor([57,  1, 46, 47, 57,  1]), target: 50\n",
      "context: tensor([57,  1, 46, 47, 57,  1, 50]), target: 53\n",
      "context: tensor([57,  1, 46, 47, 57,  1, 50, 53]), target: 60\n",
      "context: tensor([1]), target: 58\n",
      "context: tensor([ 1, 58]), target: 46\n",
      "context: tensor([ 1, 58, 46]), target: 43\n",
      "context: tensor([ 1, 58, 46, 43]), target: 56\n",
      "context: tensor([ 1, 58, 46, 43, 56]), target: 43\n",
      "context: tensor([ 1, 58, 46, 43, 56, 43]), target: 1\n",
      "context: tensor([ 1, 58, 46, 43, 56, 43,  1]), target: 41\n",
      "context: tensor([ 1, 58, 46, 43, 56, 43,  1, 41]), target: 39\n",
      "context: tensor([17]), target: 26\n",
      "context: tensor([17, 26]), target: 15\n",
      "context: tensor([17, 26, 15]), target: 17\n",
      "context: tensor([17, 26, 15, 17]), target: 10\n",
      "context: tensor([17, 26, 15, 17, 10]), target: 0\n",
      "context: tensor([17, 26, 15, 17, 10,  0]), target: 32\n",
      "context: tensor([17, 26, 15, 17, 10,  0, 32]), target: 53\n",
      "context: tensor([17, 26, 15, 17, 10,  0, 32, 53]), target: 1\n",
      "context: tensor([57]), target: 58\n",
      "context: tensor([57, 58]), target: 6\n",
      "context: tensor([57, 58,  6]), target: 1\n",
      "context: tensor([57, 58,  6,  1]), target: 61\n",
      "context: tensor([57, 58,  6,  1, 61]), target: 47\n",
      "context: tensor([57, 58,  6,  1, 61, 47]), target: 58\n",
      "context: tensor([57, 58,  6,  1, 61, 47, 58]), target: 46\n",
      "context: tensor([57, 58,  6,  1, 61, 47, 58, 46]), target: 0\n"
     ]
    }
   ],
   "source": [
    "with open(\"./input.txt\", \"r\",encoding=\"utf-8\") as file:\n",
    "    data = file.read()\n",
    "    # print(data)\n",
    "import torch\n",
    "\n",
    "voc=sorted(list(set(data)))\n",
    "# print(len(data))\n",
    "# print(list(set(data)))\n",
    "# print(set(data))\n",
    "# print(voc)\n",
    "chtoi={c:i for i,c in enumerate(voc)}\n",
    "itoch={i:c for i,c in enumerate(voc)}\n",
    "encoding=lambda s: [chtoi[c] for c in s]\n",
    "decoding=lambda s: \"\".join([itoch[c] for c in s])\n",
    "\n",
    "\n",
    "# print(encoding(\"hello world\"))\n",
    "# print(decoding(encoding(\"hello world\")))\n",
    "\n",
    "datatset=torch.tensor(encoding(data),dtype=torch.long)\n",
    "# print(datatset[:100])\n",
    "n=int(0.9*len(datatset))\n",
    "trainset=datatset[:n]\n",
    "testset=datatset[n:]\n",
    "block_size=8\n",
    "x_train=datatset[:block_size+1]\n",
    "y_train=datatset[1:block_size+1]\n",
    "# print(x_train)\n",
    "for i in range(block_size):\n",
    "    context=x_train[:i+1]\n",
    "    target=y_train[i]\n",
    "    # print(f\"context: {context}, target: {target}\")\n",
    "torch.manual_seed(42)\n",
    "batch_size=4\n",
    "block_size=8\n",
    "def getbatch(split):\n",
    "    data=trainset if split==\"train\" else testset\n",
    "    ix=torch.randint(len(data)-block_size,(batch_size,))\n",
    "    x=torch.stack(([data[i:i+block_size] for i in ix]))\n",
    "    y=torch.stack(([data[i+1:i+block_size+1] for i in ix]))\n",
    "    print(ix)\n",
    "    return x,y\n",
    "xb,yb=getbatch(\"train\")\n",
    "print(xb)\n",
    "print(yb)\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for a in range(block_size):\n",
    "        context=xb[b,:a+1]\n",
    "        target=yb[b,a]\n",
    "        print(f\"context: {context}, target: {target}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x79966c471ab0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (32) to match target batch_size (4).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m logits,loss\n\u001b[1;32m     10\u001b[0m m\u001b[38;5;241m=\u001b[39mBigramLanguageModel(\u001b[38;5;28mlen\u001b[39m(voc))\n\u001b[0;32m---> 11\u001b[0m logit,loss\u001b[38;5;241m=\u001b[39m\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m,\u001b[49m\u001b[43myb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(logit\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss)\n",
      "File \u001b[0;32m/mnt/c9542373-6611-4b24-a6c7-2979563b4e53/gptdev/gptenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c9542373-6611-4b24-a6c7-2979563b4e53/gptdev/gptenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m, in \u001b[0;36mBigramLanguageModel.forward\u001b[0;34m(self, xb, yb)\u001b[0m\n\u001b[1;32m      6\u001b[0m logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_embedding_table(xb)\n\u001b[1;32m      7\u001b[0m B,C,T\u001b[38;5;241m=\u001b[39mlogits\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m----> 8\u001b[0m loss\u001b[38;5;241m=\u001b[39m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43myb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logits,loss\n",
      "File \u001b[0;32m/mnt/c9542373-6611-4b24-a6c7-2979563b4e53/gptdev/gptenv/lib/python3.11/site-packages/torch/nn/functional.py:3494\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3493\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3494\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3495\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3499\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3500\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3501\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (32) to match target batch_size (4)."
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table=nn.Embedding(vocab_size,vocab_size)\n",
    "    def forward(self, xb,yb):\n",
    "        logits=self.token_embedding_table(xb)\n",
    "        B,C,T=logits.shape\n",
    "        loss=F.cross_entropy(logits.reshape(B*C,T),yb.reshape(B*C))\n",
    "        return logits,loss\n",
    "m=BigramLanguageModel(len(voc))\n",
    "logit,loss=m(xb,yb)\n",
    "print(logit.shape)\n",
    "print(loss)\n",
    "        \n",
    "print(len(voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gptenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
