{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[57,  1, 46, 47, 57,  1, 50, 53],\n",
      "        [ 1, 58, 46, 43, 56, 43,  1, 41],\n",
      "        [17, 26, 15, 17, 10,  0, 32, 53],\n",
      "        [57, 58,  6,  1, 61, 47, 58, 46]])\n",
      "tensor([[ 1, 46, 47, 57,  1, 50, 53, 60],\n",
      "        [58, 46, 43, 56, 43,  1, 41, 39],\n",
      "        [26, 15, 17, 10,  0, 32, 53,  1],\n",
      "        [58,  6,  1, 61, 47, 58, 46,  0]])\n",
      "context: tensor([57]), target: 1\n",
      "context: tensor([57,  1]), target: 46\n",
      "context: tensor([57,  1, 46]), target: 47\n",
      "context: tensor([57,  1, 46, 47]), target: 57\n",
      "context: tensor([57,  1, 46, 47, 57]), target: 1\n",
      "context: tensor([57,  1, 46, 47, 57,  1]), target: 50\n",
      "context: tensor([57,  1, 46, 47, 57,  1, 50]), target: 53\n",
      "context: tensor([57,  1, 46, 47, 57,  1, 50, 53]), target: 60\n",
      "context: tensor([1]), target: 58\n",
      "context: tensor([ 1, 58]), target: 46\n",
      "context: tensor([ 1, 58, 46]), target: 43\n",
      "context: tensor([ 1, 58, 46, 43]), target: 56\n",
      "context: tensor([ 1, 58, 46, 43, 56]), target: 43\n",
      "context: tensor([ 1, 58, 46, 43, 56, 43]), target: 1\n",
      "context: tensor([ 1, 58, 46, 43, 56, 43,  1]), target: 41\n",
      "context: tensor([ 1, 58, 46, 43, 56, 43,  1, 41]), target: 39\n",
      "context: tensor([17]), target: 26\n",
      "context: tensor([17, 26]), target: 15\n",
      "context: tensor([17, 26, 15]), target: 17\n",
      "context: tensor([17, 26, 15, 17]), target: 10\n",
      "context: tensor([17, 26, 15, 17, 10]), target: 0\n",
      "context: tensor([17, 26, 15, 17, 10,  0]), target: 32\n",
      "context: tensor([17, 26, 15, 17, 10,  0, 32]), target: 53\n",
      "context: tensor([17, 26, 15, 17, 10,  0, 32, 53]), target: 1\n",
      "context: tensor([57]), target: 58\n",
      "context: tensor([57, 58]), target: 6\n",
      "context: tensor([57, 58,  6]), target: 1\n",
      "context: tensor([57, 58,  6,  1]), target: 61\n",
      "context: tensor([57, 58,  6,  1, 61]), target: 47\n",
      "context: tensor([57, 58,  6,  1, 61, 47]), target: 58\n",
      "context: tensor([57, 58,  6,  1, 61, 47, 58]), target: 46\n",
      "context: tensor([57, 58,  6,  1, 61, 47, 58, 46]), target: 0\n"
     ]
    }
   ],
   "source": [
    "with open(\"./input.txt\", \"r\",encoding=\"utf-8\") as file:\n",
    "    data = file.read()\n",
    "    # print(data)\n",
    "import torch\n",
    "\n",
    "voc=sorted(list(set(data)))\n",
    "# print(len(data))\n",
    "# print(list(set(data)))\n",
    "# print(set(data))\n",
    "# print(voc)\n",
    "chtoi={c:i for i,c in enumerate(voc)}\n",
    "itoch={i:c for i,c in enumerate(voc)}\n",
    "encoding=lambda s: [chtoi[c] for c in s]\n",
    "decoding=lambda s: \"\".join([itoch[c] for c in s])\n",
    "\n",
    "\n",
    "# print(encoding(\"hello world\"))\n",
    "# print(decoding(encoding(\"hello world\")))\n",
    "\n",
    "datatset=torch.tensor(encoding(data),dtype=torch.long)\n",
    "# print(datatset[:100])\n",
    "n=int(0.9*len(datatset))\n",
    "trainset=datatset[:n]\n",
    "testset=datatset[n:]\n",
    "block_size=8\n",
    "x_train=datatset[:block_size+1]\n",
    "y_train=datatset[1:block_size+1]\n",
    "# print(x_train)\n",
    "for i in range(block_size):\n",
    "    context=x_train[:i+1]\n",
    "    target=y_train[i]\n",
    "    # print(f\"context: {context}, target: {target}\")\n",
    "torch.manual_seed(42)\n",
    "batch_size=4\n",
    "block_size=8\n",
    "def getbatch(split):\n",
    "    data=trainset if split==\"train\" else testset\n",
    "    ix=torch.randint(len(data)-block_size,(batch_size,))\n",
    "    x=torch.stack(([data[i:i+block_size] for i in ix]))\n",
    "    y=torch.stack(([data[i+1:i+block_size+1] for i in ix]))\n",
    "    # print(ix)\n",
    "    return x,y\n",
    "xb,yb=getbatch(\"train\")\n",
    "print(xb)\n",
    "print(yb)\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for a in range(block_size):\n",
    "        context=xb[b,:a+1]\n",
    "        target=yb[b,a]\n",
    "        print(f\"context: {context}, target: {target}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(42)\n",
    "n_embd=32\n",
    "block_size=8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" One head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size,n_embd, block_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape  \n",
    "\n",
    "      \n",
    "        k = self.key(x)  \n",
    "        q = self.query(x)  \n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5  \n",
    "        \n",
    "        # Apply the triangular mask\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  \n",
    "        \n",
    "        wei = F.softmax(wei, dim=-1)  \n",
    "\n",
    "        v = self.value(x)  \n",
    "        out = wei @ v \n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x8 and 32x65)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 52\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m idx\n\u001b[1;32m     51\u001b[0m m\u001b[38;5;241m=\u001b[39mBigramLanguageModel(\u001b[38;5;28mlen\u001b[39m(voc))\n\u001b[0;32m---> 52\u001b[0m logit,loss\u001b[38;5;241m=\u001b[39m\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m,\u001b[49m\u001b[43myb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# print(logit.shape)\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# print(loss)\u001b[39;00m\n\u001b[1;32m     55\u001b[0m         \n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# print(len(voc))\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(decoding(m\u001b[38;5;241m.\u001b[39mgenerate(idx\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m),dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong),max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()))\n",
      "File \u001b[0;32m/mnt/c9542373-6611-4b24-a6c7-2979563b4e53/gptdev/gptenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c9542373-6611-4b24-a6c7-2979563b4e53/gptdev/gptenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[29], line 16\u001b[0m, in \u001b[0;36mBigramLanguageModel.forward\u001b[0;34m(self, xb, yb)\u001b[0m\n\u001b[1;32m     14\u001b[0m x\u001b[38;5;241m=\u001b[39mtoken_embedding\u001b[38;5;241m+\u001b[39mpos_emb\n\u001b[1;32m     15\u001b[0m x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msa_head(x)\n\u001b[0;32m---> 16\u001b[0m logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m yb \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m     B,C,T\u001b[38;5;241m=\u001b[39mlogits\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m/mnt/c9542373-6611-4b24-a6c7-2979563b4e53/gptdev/gptenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c9542373-6611-4b24-a6c7-2979563b4e53/gptdev/gptenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/mnt/c9542373-6611-4b24-a6c7-2979563b4e53/gptdev/gptenv/lib/python3.11/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x8 and 32x65)"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table=nn.Embedding(vocab_size,n_embd)\n",
    "        self.positional_embedding_table=nn.Embedding(block_size,n_embd) # plus one for the initial padding token\n",
    "        self.sa_head=Head( 32,n_embd,block_size)\n",
    "    \n",
    "        self.lm_head=nn.Linear(n_embd,vocab_size)\n",
    "    def forward(self, xb,yb=None):\n",
    "        B,T=xb.shape\n",
    "        token_embedding=self.token_embedding_table(xb)\n",
    "        pos_emb=self.token_embedding_table(torch.arange(T))\n",
    "        # logits=self.lm_head(token_embedding)\n",
    "        x=token_embedding+pos_emb\n",
    "        x=self.sa_head(x)\n",
    "        logits=self.lm_head(x)\n",
    "      \n",
    "        if yb is not None:\n",
    "            B,C,T=logits.shape\n",
    "            logits=logits.view(B*C,T)\n",
    "            yb=yb.view(B*C)\n",
    "\n",
    "            loss=F.cross_entropy(logits,yb)\n",
    "        else:\n",
    "            loss=None\n",
    "        return logits,loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens): \n",
    "        # idx is (B, T) array of indices in the current context \n",
    "        for _ in range(max_new_tokens):  \n",
    "            # get the predictions \n",
    "            logits, loss = self(idx)  \n",
    "\n",
    "            # focus only on the last time step \n",
    "            # print(logits)\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)  \n",
    "            # print(logits)\n",
    "\n",
    "            # apply softmax to get probabilities  \n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)  \n",
    "\n",
    "            # sample from the distribution  \n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)  \n",
    "\n",
    "            # append sampled index to the running sequence  \n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1) \n",
    "            # print(idx) \n",
    "\n",
    "        return idx\n",
    "\n",
    "m=BigramLanguageModel(len(voc))\n",
    "logit,loss=m(xb,yb)\n",
    "# print(logit.shape)\n",
    "# print(loss)\n",
    "        \n",
    "# print(len(voc))\n",
    "\n",
    "print(decoding(m.generate(idx=torch.zeros((1,1),dtype=torch.long),max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0, 16, 23, 34, 39, 51,  4, 23,  9, 38, 52, 49, 21, 25, 39, 55, 52, 58,\n",
       "         48, 46,  7, 13, 35,  5, 14, 22, 62, 62, 45, 46, 48, 38, 32, 14, 15, 52,\n",
       "         15, 33, 12, 55, 14, 29, 40, 61, 63, 15, 48, 46, 30, 46, 31, 24, 63, 22,\n",
       "         44, 19, 58, 57, 59, 58, 22, 14, 63,  7, 48,  4,  6, 56,  6,  5, 49,  3,\n",
       "         28, 32, 59, 38, 51,  9, 58, 28, 40, 59,  1, 26, 61, 20,  7, 21, 28, 43,\n",
       "         39, 55, 59, 22, 34, 56, 23,  5, 62,  1, 44]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.generate(idx=torch.zeros((1,1),dtype=torch.long),max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"\"\"gBXCYUEBUkTrhlwhYvCHU3jVQWIOVFHSbBtlnSfFoHZK.nHR&AcjvfzzJ.K,HCYUui-xd\n",
    "L;Wu3VnSgv.$afs\n",
    "BqpdHVLgwcOwN \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.AdamW(m.parameters(),lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Loss: 4.6977\n",
      "Step 101, Loss: 4.5542\n",
      "Step 201, Loss: 4.4854\n",
      "Step 301, Loss: 4.3880\n",
      "Step 401, Loss: 4.2180\n",
      "Step 501, Loss: 4.2192\n",
      "Step 601, Loss: 4.0598\n",
      "Step 701, Loss: 3.9743\n",
      "Step 801, Loss: 3.9993\n",
      "Step 901, Loss: 3.9300\n",
      "Step 1001, Loss: 3.7942\n",
      "Step 1101, Loss: 3.8274\n",
      "Step 1201, Loss: 3.5697\n",
      "Step 1301, Loss: 3.4974\n",
      "Step 1401, Loss: 3.5198\n",
      "Step 1501, Loss: 3.3823\n",
      "Step 1601, Loss: 3.4163\n",
      "Step 1701, Loss: 3.3589\n",
      "Step 1801, Loss: 3.3072\n",
      "Step 1901, Loss: 3.2026\n",
      "Step 2001, Loss: 3.1628\n",
      "Step 2101, Loss: 3.0630\n",
      "Step 2201, Loss: 3.1235\n",
      "Step 2301, Loss: 2.9732\n",
      "Step 2401, Loss: 3.0888\n",
      "Step 2501, Loss: 2.9927\n",
      "Step 2601, Loss: 2.8689\n",
      "Step 2701, Loss: 2.9053\n",
      "Step 2801, Loss: 2.9279\n",
      "Step 2901, Loss: 2.8635\n",
      "Step 3001, Loss: 2.8600\n",
      "Step 3101, Loss: 2.9175\n",
      "Step 3201, Loss: 2.7034\n",
      "Step 3301, Loss: 2.7863\n",
      "Step 3401, Loss: 2.7986\n",
      "Step 3501, Loss: 2.6293\n",
      "Step 3601, Loss: 2.7755\n",
      "Step 3701, Loss: 2.7048\n",
      "Step 3801, Loss: 2.6091\n",
      "Step 3901, Loss: 2.5815\n",
      "Step 4001, Loss: 2.6208\n",
      "Step 4101, Loss: 2.6359\n",
      "Step 4201, Loss: 2.6495\n",
      "Step 4301, Loss: 2.6701\n",
      "Step 4401, Loss: 2.6540\n",
      "Step 4501, Loss: 2.6137\n",
      "Step 4601, Loss: 2.5385\n",
      "Step 4701, Loss: 2.7403\n",
      "Step 4801, Loss: 2.5221\n",
      "Step 4901, Loss: 2.6266\n",
      "Step 5001, Loss: 2.5505\n",
      "Step 5101, Loss: 2.5610\n",
      "Step 5201, Loss: 2.5470\n",
      "Step 5301, Loss: 2.5149\n",
      "Step 5401, Loss: 2.5262\n",
      "Step 5501, Loss: 2.4623\n",
      "Step 5601, Loss: 2.5615\n",
      "Step 5701, Loss: 2.6202\n",
      "Step 5801, Loss: 2.5995\n",
      "Step 5901, Loss: 2.5212\n",
      "Step 6001, Loss: 2.5571\n",
      "Step 6101, Loss: 2.5108\n",
      "Step 6201, Loss: 2.5454\n",
      "Step 6301, Loss: 2.4263\n",
      "Step 6401, Loss: 2.4940\n",
      "Step 6501, Loss: 2.4318\n",
      "Step 6601, Loss: 2.6770\n",
      "Step 6701, Loss: 2.5536\n",
      "Step 6801, Loss: 2.4955\n",
      "Step 6901, Loss: 2.4084\n",
      "Step 7001, Loss: 2.3854\n",
      "Step 7101, Loss: 2.5597\n",
      "Step 7201, Loss: 2.5068\n",
      "Step 7301, Loss: 2.4465\n",
      "Step 7401, Loss: 2.4949\n",
      "Step 7501, Loss: 2.3700\n",
      "Step 7601, Loss: 2.4280\n",
      "Step 7701, Loss: 2.4072\n",
      "Step 7801, Loss: 2.3597\n",
      "Step 7901, Loss: 2.4519\n",
      "Step 8001, Loss: 2.5282\n",
      "Step 8101, Loss: 2.4408\n",
      "Step 8201, Loss: 2.5789\n",
      "Step 8301, Loss: 2.4648\n",
      "Step 8401, Loss: 2.5081\n",
      "Step 8501, Loss: 2.5864\n",
      "Step 8601, Loss: 2.4633\n",
      "Step 8701, Loss: 2.5087\n",
      "Step 8801, Loss: 2.4265\n",
      "Step 8901, Loss: 2.5012\n",
      "Step 9001, Loss: 2.4003\n",
      "Step 9101, Loss: 2.4489\n",
      "Step 9201, Loss: 2.4928\n",
      "Step 9301, Loss: 2.4946\n",
      "Step 9401, Loss: 2.4609\n",
      "Step 9501, Loss: 2.4100\n",
      "Step 9601, Loss: 2.4860\n",
      "Step 9701, Loss: 2.4334\n",
      "Step 9801, Loss: 2.5789\n",
      "Step 9901, Loss: 2.5258\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "for steps in range(10000):\n",
    "    xb, yb = getbatch('train')\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if steps % 100 == 0:\n",
    "        print(f\"Step {steps+1}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RY:\n",
      "Yors t p kere isd, m INGea earusher Tistced.\n",
      "Wharologo!\n",
      "\n",
      "\n",
      "Whe hildy por byof?\n",
      "GUFarastaldXIndesw\n"
     ]
    }
   ],
   "source": [
    "print(decoding(m.generate(idx=torch.zeros((1,1),dtype=torch.long),max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.9269,  1.4873],\n",
      "         [ 0.9007, -2.1055],\n",
      "         [ 0.6784, -1.2345],\n",
      "         [-0.0431, -1.6047],\n",
      "         [-0.7521,  1.6487],\n",
      "         [-0.3925, -1.4036],\n",
      "         [-0.7279, -0.5594],\n",
      "         [-0.7688,  0.7624]],\n",
      "\n",
      "        [[ 1.6423, -0.1596],\n",
      "         [-0.4974,  0.4396],\n",
      "         [-0.7581,  1.0783],\n",
      "         [ 0.8008,  1.6806],\n",
      "         [ 1.2791,  1.2964],\n",
      "         [ 0.6105,  1.3347],\n",
      "         [-0.2316,  0.0418],\n",
      "         [-0.2516,  0.8599]],\n",
      "\n",
      "        [[-1.3847, -0.8712],\n",
      "         [-0.2234,  1.7174],\n",
      "         [ 0.3189, -0.4245],\n",
      "         [ 0.3057, -0.7746],\n",
      "         [-1.5576,  0.9956],\n",
      "         [-0.8798, -0.6011],\n",
      "         [-1.2742,  2.1228],\n",
      "         [-1.2347, -0.4879]],\n",
      "\n",
      "        [[-0.9138, -0.6581],\n",
      "         [ 0.0780,  0.5258],\n",
      "         [-0.4880,  1.1914],\n",
      "         [-0.8140, -0.7360],\n",
      "         [-1.4032,  0.0360],\n",
      "         [-0.0635,  0.6756],\n",
      "         [-0.0978,  1.8446],\n",
      "         [-1.1845,  1.3835]]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "B,T,C=4,8,2\n",
    "x=torch.randn(B,T,C)\n",
    "xbow=torch.zeros(B,T,C)\n",
    "print(x)\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev=x[b,:t+1]\n",
    "        xbow[b,t]=torch.mean(xprev,dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.9269,  1.4873],\n",
       "         [ 0.9007, -2.1055],\n",
       "         [ 0.6784, -1.2345],\n",
       "         [-0.0431, -1.6047],\n",
       "         [-0.7521,  1.6487],\n",
       "         [-0.3925, -1.4036],\n",
       "         [-0.7279, -0.5594],\n",
       "         [-0.7688,  0.7624]]),\n",
       " tensor([[ 1.9269,  1.4873],\n",
       "         [ 1.4138, -0.3091],\n",
       "         [ 1.1687, -0.6176],\n",
       "         [ 0.8657, -0.8644],\n",
       "         [ 0.5422, -0.3617],\n",
       "         [ 0.3864, -0.5354],\n",
       "         [ 0.2272, -0.5388],\n",
       "         [ 0.1027, -0.3762]]))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0],xbow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "a=torch.tril(torch.ones(3,3,dtype=torch.float))\n",
    "b=torch.randint(0,10,(3,2),dtype=torch.float)\n",
    "c=a@b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 0., 0.],\n",
       "         [1., 1., 0.],\n",
       "         [1., 1., 1.]]),\n",
       " tensor([[2., 7.],\n",
       "         [6., 4.],\n",
       "         [6., 5.]]))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=a/torch.sum(a,dim=1,keepdim=True)\n",
    "# b=torch.sum(a,dim=1)\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v=torch.tril(torch.ones(T,T))\n",
    "v=v/torch.sum(v,dim=1,keepdim=True)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s=v/torch.sum(v,dim=1)\n",
    "# s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 8]), torch.Size([4, 8, 2]))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei=torch.tril(torch.ones(T,T))\n",
    "wei=wei/torch.sum(wei,dim=1,keepdim=True)\n",
    "xbow2=wei@x\n",
    "wei.shape,x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow2.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tril=torch.tril(torch.ones(T,T))\n",
    "wei=torch.zeros(T,T)\n",
    "# tril,wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei=wei.masked_fill(tril==0,float(\"-inf\"))\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei=torch.softmax(wei,dim=1)\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "B,T,C=4,8,32\n",
    "x=torch.randn(B,T,C)\n",
    "head_size=16\n",
    "key=nn.Linear(C,head_size,bias=False)\n",
    "query=nn.Linear(C,head_size,bias=False)\n",
    "value=nn.Linear(C,head_size,bias=False)\n",
    "k=key(x)\n",
    "q=query(x)\n",
    "wei=q@k.transpose(-2,-1)\n",
    "\n",
    "# tril=torch.tril(torch.ones(T,T))\n",
    "# wei=torch.zeros(T,T)\n",
    "wei=wei.masked_fill(tril==0,float(\"-inf\"))\n",
    "wei=torch.softmax(wei,dim=1)\n",
    "v=value(x)\n",
    "out=wei@v\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gptenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
